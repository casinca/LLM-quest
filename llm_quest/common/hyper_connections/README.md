TODO

TODO Check latex on github rendering


## Hyper-connections

Hyper-connections is an alternative to the traditional residual/skip connections that we've seen in transformers since
it's introduction in the ResNet paper.

For example instead of having for a single layer of a transformer block the usual output as:

$$\mathbf{x}_{l+1} = \mathbf{x}_l + \mathcal{F}(\mathbf{x}_l, \mathcal{W}_l)$$

with:
*   **$\mathbf{x}_l$**: the untouched input residual/skip connection (the identity mapping as DeepSeek says)
*   **$\mathcal{F}(\mathbf{x}_l, \mathcal{W}_l)$**: the residual/layer modifications from the trf block (Attention,
    ffn/MoE...)

The untouched input is replaced by a "hyper-connection" $\mathcal{H}_l^{\text{res}}\mathbf{x}_l$, which is
basically a learnable matrix/weight $\mathcal{H}_l^{\text{res}}$ scaling the untouched input $\mathbf{x}_l$ (*mixes features within the residual stream* as DeepSeek
put it).

TODO mention Hpost Hpre

$$\mathbf{x}_{l+1} = \mathcal{H}_l^{\text{res}}\mathbf{x}_l + \mathcal{H}_l^{\text{post} \top} \mathcal{F}(\mathcal{H}_l^{\text{pre}} \mathbf{x}_l, \mathcal{W}_l),$$

&nbsp;

### To begin with, why is it called Hyper-connections and what is $\mathcal{H}_l^{\text{res}}$?

The term is derived from HyperNetworks, which as the original paper abtracts mentions: Hypernets are neural networks
that generate the weights of other neural networks.

Let's take the example of a traditional single linear layer without bias, we have:

$$
y = W \cdot x
$$

- During training, the weights in $\mathbf{W}$ are not fixed (they are learned through backprop) but during
  inference, they are set in stones. So we have fixed weights.

For an "HyperLinear" layer, instead of having a fixed weight matrix $\mathbf{W}$, we have a 2nd small linear layer
$H(x)$ that learns to generate a weight matrix $W$ on the fly during inference.

so we end up with:

$$y = H(x) \cdot x$$

- whether during training or inference, the weights $W$ here are not fixed, they are generated by $H(x)$

&nbsp;

To make the parallel with hyper-connections, In our case $H(x)$ is $\mathcal{H}_l^{\text{res}}$ from the mHC paper eq 5.

Which is the result of a single layer ffn that is tanh activated, with a scaling factor $\alpha$ and a bias $\mathbf{b}$:
 $$\mathcal{H}^{\text{res}} = \alpha \cdot \tanh(\theta \mathbf{x}^\top) + \mathbf{b}$$

&nbsp;

Hence the term hyperconnections, instead of having a fixed weight matrix $W$ that could have scaled/mixed features
within the residual stream as $\mathbf{x}_{l+1} = W \cdot \mathbf{x}_l$, we have a small neural network that
generates the weights $\mathcal{H}_l^{\text{res}}$ on the fly $\mathbf{x}_{l+1} = \mathcal{H}_l^{\text{res}} \cdot \mathbf{x}_l$

A good? analogy would be to compare it with LoRA (omitting the goal efficiency of LoRA), where we can manually swap the
fixed low rank weights $BA$ for another set without retraining the model. Here similarly the weights are dynamically
changed/generated on the fly during inference based on the tokens and what has been learned during training.

TODO need to better reformulate the loRA analogy


&nbsp;

## Acknowledgements

- [Hyper-connections](https://arxiv.org/abs/2409.19606)
- [DeepSeek mHC](https://arxiv.org/abs/2512.24880)
- [HyperNetworks](https://arxiv.org/abs/1609.09106)
- [ResNet](https://arxiv.org/abs/1512.03385)

