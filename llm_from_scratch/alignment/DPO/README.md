# Direct Preference Optimization (DPO) step by step

There weren't many details on the DPO impl, for brevity I presume as it can quickly become verbose in a notebook. So I wrote
here my dissection of the paper: https://arxiv.org/abs/2305.18290 on how we end up, step by step, to the DPO loss
function. Concerning the code, it follows the structure of @rasbt impl and the paper.

---

DPO is a lighter, more efficient approach for aligning LLMs with human preferences where only 2 models are needed vs classic RLHF.  
*Rafailov et al.* cleverly reparametrize the reward function directly based on the reference and policy models
eliminating the need for a reward model.  
They use a Bradley-Terry preference model and transforms the problem into a Maximum Likelihood Estimation (MLE) task.

&nbsp;

The Bradley-Terry (BT) model estimates the probability/strength of pair comparisons, used in ELO rankings for instance.

With a pair $(i, j)$ and their scores $p_i$ and $p_j$ we can express the BT model as:

$$
P(i > j) = \frac{p_i}{p_i + p_j} \qquad  (1)
$$
 
Ie, the probability that $i$ is preferred (or ranks higher or wins) over $j$.

Here we parametrize the BT model with instead exponential scores functions $p_i = e^{\beta_i}$ and $p_j = e^{\beta_j}$:

$$
P(i > j) = \frac{e^{\beta_i}}{e^{\beta_i} + e^{\beta_j}} = \frac{1}{1 + e^{\beta_j - \beta_i}} \qquad  (2)
$$

We can already appreciate the similarity with the sigmoid function, with $x=β_j−β_i$:

$$
\sigma(\beta_j - \beta_i) = \frac{1}{1 + e^{-(\beta_j - \beta_i)}} = \frac{1}{1 + e^{\beta_i-\beta_j}}  \qquad  (3)
$$

Which is equivalent to:

$$
P(i > j) = \sigma(\beta_i - \beta_j) \qquad  (4)
$$

*How does it relate to DPO?*  

Instead of comparing a pair of (teamA, teamB) for example sport, in our case, scores $\beta_j$ and $\beta_i$ correspond
to reward values/scores assigned by a reward function $r(x,y)$ that evaluates the quality of different responses.
Specifically, $r(x,y1)$ and $r(x,y2)$ represent the reward values for the preferred
response $y_1$ and rejected response $y_2$​ given a prompt $x$ generated by an SFT model (the reference model
$\pi_{ref}$, note that at the start reference model = policy model, so doesn't really matter​)

We can replace our scores $r(x,y_1)$ and $r(x,y_2)$ in (4):

$$
P(y_1 > y_2\mid x) = \sigma(r(x,y_1) - r(x,y_2)) \qquad  (5)
$$

If we frame our BT model as an MLE problem in a binary classification case (preferred/rejected), we can turn it into an
NLL loss.

The binary cross entropy loss is:

$$
L_{BCE} = -\left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right] \qquad  (6)
$$

 - With $y_i$ the true label (1 for positive class, 0 for negative class), $p_i$ is the predicted probability of the
positive class

Substituting BT (5) into the BCE loss: 

$$
L_{BCE} =  -\left[y_i \log(\sigma(r(x,y_1) - r(x,y_2))) + (1 - y_i) \log(1 - [\sigma(r(x,y_1) - r(x,y_2))])\right] 
\qquad  (7)
$$

since we know that $y_i$ is 1 for the preferred response, we can simplify it to:

$$
L_{BCE} = -\left[\log(\sigma(r(x,y_1) - r(x,y_2)))\right] \qquad  (8)
$$

Which is the same as trying to minimize an NLL:

$$
 \mathcal{L}(r) = -\left[\log \sigma(r(x,y_1) - r(x,y_2))\right] \qquad  (9)
$$

(to be more rigorous like the paper, the $\mathbb{E}$ xpectation over all pairs for the whole dataset $D$ where $(x,y_1,y_2)$ are
sampled from:)

$$
    \mathcal{L}(r) = -\mathbb{E}_{(x,y_1,y_2) \sim D} \left[ \log \sigma(r(x,y_1) - r(x,y_2)) \right] \qquad  (10)
$$

&nbsp;

We get back the main frame for our DPO loss function from the paper, but it's not quite that yet.  
We need to define our reward function $r(x,y)$.

$r(x,y)$ is taken from the RLHF equation where we want to maximize the reward function $r(x,y)$ while minimizing
divergence between the policy model $\pi_\theta$ and the reference model $\pi_{ref}$ via a relative entropy
(Kullback–Leibler div) penalty *(not rendered properly)*:

$$
\underset{\pi_\theta}{\max} \underset{\pi_\theta(y|x)}{\mathbb{E}} [ r(x, y) ] - \beta D_{KL} [ \pi_\theta(y|x) \parallel \pi_{ref}(y|x) ] \qquad  (11)
$$

 - $\beta$ is a hparam to scale the trade-off between the reward and KL divergence terms.
 - $\pi_\theta(y|x)$ the probability that the policy model $\pi_\theta$ gives the exact response $y$ given the prompt
   $x$ (the proba of a response $y$, is the product of the probas of the tokens in $y$, ie 
   $P(y|x) = P(y_1|x) \times P(y_2|x,y_1) \times P(y_3|x,y_1,y_2) \times \ldots \times P(y_n|x,y_1,\ldots,y_{n-1})$ )  
 - $\pi_{ref}(y|x)$ same as $\pi_\theta(y|x)$ but for the reference model

Without going into the full derivation of (10) with Lagrange multiplier, we want to derive it wrt the policy
${\pi_\theta}$ and also solve for ${\pi_\theta}$ which gives the result:

$$
\pi_\theta(y | x) = \frac{1}{Z(x)} \cdot \pi_{ref}(y | x) \cdot e^{\left(\frac{1}{\beta} r(x, y)\right)} \tag{12}
$$

rearranged to isolate the reward term:

$$
r(x, y) = \beta \log \left( \frac{\pi_\theta(y \mid x)}{\pi_{ref}(y \mid x)} \right) + \beta \log Z(x) \qquad  (13)
$$

So the reward function $r(x,y)$ is a scalable log ratio of probabilities between the policy model $\pi_\theta$ and the
reference model $\pi_{ref}$, plus a term $Z(x)$ that is the partition function (not going into details as we won't need
$Z(x)$, see (14) below.)

We want to maximize that log prob ratio for the preferred response $y_1$, ie having a higher prob of generating that
$y_1$ response for $\pi_\theta$ than $\pi_{ref}$ (baseline) and the opposite for the rejected response $y_2$.

In our case of preferred response $y_1$ and rejected response $y_2$, the $Z(x)$ terms will cancel
out for $r(x,y_1) - r(x,y_2)$:

$$ 
r(x,y_1) - r(x,y_2) = \beta \log \left( \frac{\pi_\theta(y_1 \mid x)}{\pi_{ref}(y_1 \mid x)} \right) + \cancel{\beta
\log Z(x)} - \beta \log \left(\frac{\pi_\theta(y_2 \mid x)}{\pi_{ref}(y_2 \mid x)} \right) - \cancel{\beta \log Z(x)}  
\qquad  (14)
$$

Plugging our reward scores back into BT (5) gives:

$$
P(y_1 > y_2 \mid x) = \sigma \left( \beta \log \left( \frac{\pi_\theta(y_1 \mid x)}{\pi_{ref}(y_1 \mid x)} \right) -
\beta \log \left( \frac{\pi_\theta(y_2 \mid x)}{\pi_{ref}(y_2 \mid x)} \right) \right) \qquad  (15)
$$

and finally into our loss (9) for minimization:

$$
L_{DPO} = - \left[ \log \sigma \left( \beta \log \left( \frac{\pi_\theta(y_1 \mid x)}{\pi_{ref}(y_1 \mid x)}
\right) - \beta \log \left( \frac{\pi_\theta(y_2 \mid x)}{\pi_{ref}(y_2 \mid x)} \right) \right) \right] \qquad  (16)
$$
